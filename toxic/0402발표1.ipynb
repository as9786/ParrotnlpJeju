{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0402발표1.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Sbr-Q4tm1VLvV2npeTZI7RTIUCamaPum",
      "authorship_tag": "ABX9TyP20jCIk9hergCb/F7x5YYB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/as9786/ParrotnlpJeju/blob/main/0402%EB%B0%9C%ED%91%9C1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1hY5rZScx2C"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Embedding, Input, GRU, Bidirectional, GlobalMaxPooling1D, Dropout, Flatten,SpatialDropout1D,Conv1D,concatenate\n",
        "from tensorflow.keras.layers import GlobalAveragePooling1D, add, ReLU\n",
        "#Bidirectional은 양방향으로 데이터를 보기 위해서 사용되는 함수\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AdZeGdNd19c"
      },
      "source": [
        "train_df = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rH566ob7d2f-"
      },
      "source": [
        "X_train = train_df.comment_text\n",
        "y_train = train_df[[\"toxic\",\t\"severe_toxic\",\t\"obscene\",\t\"threat\",\t\"insult\",\t\"identity_hate\"]].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVSiL0jXeDr7"
      },
      "source": [
        "num_words = 70000\n",
        "max_len = 150"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjVqHvtbeIPg"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=num_words)\n",
        "tokenizer.fit_on_texts(list(X_train))\n",
        "word_index = tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b27QZQq4eQ23"
      },
      "source": [
        "# tokenizer의 num_words가 단어의 개수를 제어해주지 못하기 때문에 인위적으로 단어 개수 제한\n",
        "word_index = {e:i for e,i in word_index.items() if i <= num_words}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxzroFzOeWPU"
      },
      "source": [
        "len(word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yji8GbYxeZj9"
      },
      "source": [
        "X_train = tokenizer.texts_to_sequences(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssbuRCa0ec3T"
      },
      "source": [
        "X_train = sequence.pad_sequences(X_train, maxlen = max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "secAQfE2eeOg"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jJOG3JmehOs"
      },
      "source": [
        "n=0\n",
        "f = open('glove.6B.300d.txt', encoding=\"utf8\") # 단어를 300차원으로 표현\n",
        "\n",
        "for line in f:\n",
        "    word_vector = line.split() # 각 줄을 읽어와서 word_vector에 저장.\n",
        "    print(word_vector) # 각 줄을 출력\n",
        "    word = word_vector[0] # word_vector에서 첫번째 값만 저장\n",
        "    print(word) # word_vector의 첫번째 값만 출력\n",
        "    n=n+1\n",
        "    if n==2:\n",
        "        break\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1H68jfqrfa_O"
      },
      "source": [
        "\n",
        "print(type(word_vector))\n",
        "print(len(word_vector))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1-Ly3dsfdYC"
      },
      "source": [
        "import numpy as np\n",
        "embedding_dict = dict()\n",
        "f = open('glove.6B.300d.txt', encoding=\"utf8\")\n",
        "\n",
        "for line in f:\n",
        "    word_vector = line.split()\n",
        "    word = word_vector[0]\n",
        "    word_vector_arr = np.asarray(word_vector[1:], dtype='float32') # 300개의 값을 가지는 array로 변환\n",
        "    embedding_dict[word] = word_vector_arr\n",
        "f.close()\n",
        "print('%s개의 Embedding vector가 있습니다.' % len(embedding_dict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GijfO0hffEq"
      },
      "source": [
        "print(embedding_dict['respectable'])\n",
        "print(len(embedding_dict['respectable']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ka0BUYs3fvEX"
      },
      "source": [
        "embedding_matrix = np.zeros((num_words, 300))\n",
        "# 단어 집합 크기의 행과 300개의 열을 가지는 행렬 생성. 값은 전부 0으로 채워진다.\n",
        "np.shape(embedding_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NY_szgJIfvZv"
      },
      "source": [
        "embedding_matrix[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIGyJ_qrfw9L"
      },
      "source": [
        "for word, i in word_index.items():\n",
        "  tmp = embedding_dict.get(word)\n",
        "  if tmp is not None:\n",
        "    embedding_matrix[i-1] = tmp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mREtdd14fzjl"
      },
      "source": [
        "input = Input(shape=(max_len, )) \n",
        "layer = Embedding(num_words, 300, weights=[embedding_matrix], input_length=max_len, trainable=True)(input)\n",
        "layer = SpatialDropout1D(0.2)(layer)\n",
        "layer = Bidirectional(GRU(128, return_sequences=True))(layer)\n",
        "layer = Conv1D(128, kernel_size = 3)(layer)   \n",
        "hidden = concatenate([GlobalMaxPooling1D()(layer),GlobalAveragePooling1D()(layer)])\n",
        "hidden = Dropout(0.25)(hidden)\n",
        "result= Dense(6, activation=\"sigmoid\")(hidden)\n",
        "model = Model(inputs = input, outputs = result)  \n",
        "model.summary()\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaxMOF7WhBYl"
      },
      "source": [
        "hist = model.fit(X_train, y_train, batch_size=512, epochs=4, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaStxmmWjbb5"
      },
      "source": [
        "model.save('/content/drive/MyDrive/toxic7_model0401.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2n3SGTVjl5f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def acc_loss_plot(hist):\n",
        "    fig, loss_ax = plt.subplots()\n",
        "    acc_ax = loss_ax.twinx()\n",
        "\n",
        "    loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "    loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
        "    loss_ax.set_xlabel('epoch')\n",
        "    loss_ax.set_ylabel('loss')\n",
        "    loss_ax.legend(loc = 'upper left')\n",
        "\n",
        "    acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
        "    acc_ax.plot(hist.history['val_accuracy'], 'g', label='val acc')\n",
        "    acc_ax.set_ylabel('accuracy')\n",
        "    acc_ax.legend(loc='upper right')\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEHERaiGjmj1"
      },
      "source": [
        "acc_loss_plot(hist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onW9-O9RjqzV"
      },
      "source": [
        "test_df = pd.read_csv('/content/drive/MyDrive/test.csv')\n",
        "test_df = test_df['comment_text']\n",
        "test_final = tokenizer.texts_to_sequences(test_df)\n",
        "test_padded =sequence.pad_sequences(test_final, maxlen=150)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0Qq4cBDjsnH"
      },
      "source": [
        "predict = model.predict(test_padded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlAaKxa1jvkr"
      },
      "source": [
        "submission = pd.read_csv('/content/drive/MyDrive/sample_submission.csv')\n",
        "submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = predict\n",
        "submission.to_csv('/content/drive/MyDrive/submission7.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
